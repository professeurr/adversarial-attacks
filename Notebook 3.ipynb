{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Notebook 3.ipynb","provenance":[],"collapsed_sections":["pejwlPboMtOj","rcYrp0VvB-tC","RV7GnBudCcpr"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"id":"YroMaxFATRHG","executionInfo":{"status":"ok","timestamp":1607679169633,"user_tz":-60,"elapsed":868,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 1\n","\n","import numpy as np\n","import scipy as sp\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiY3BiAiTRHK"},"source":["[CELL ID] 2\n","## From adversarial examples to training robust models\n","\n","In the previous notebooks, we focused on methods for solving the maximization problem over perturbations; that is, to finding the solution to the problem\n","\\begin{equation}\n","\\DeclareMathOperator*{\\maximize}{maximize}\n","\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n","\\end{equation}\n","\n","In this notebook, we will focus on training a robust classifier. More precisly, we aim at solving following minimization problem, namely Adversarial Training:\n","\\begin{equation}\n","\\DeclareMathOperator*{\\minimize}{minimize}\n","\\minimize_\\theta \\frac{1}{|S|} \\sum_{x,y \\in S} \\max_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n","\\end{equation}\n","The order of the min-max operations is important here.  Specially, the max is inside the minimization, meaning that the adversary (trying to maximize the loss) gets to \"move\" _second_.  We assume, essentially, that the adversary has full knowledge of the classifier parameters $\\theta$, and that they get to specialize their attack to whatever parameters we have chosen in the outer minimization. The goal of the robust optimization formulation, therefore, is to ensure that the model cannot be attacked _even if_ the adversary has full knowledge of the model.  Of course, in practice we may want to make assumptions about the power of the adversary but it can be difficult to pin down a precise definition of what we mean by the \"power\" of the adversary, so extra care should be taken in evaluating models against possible \"realistic\" adversaries."]},{"cell_type":"markdown","metadata":{"id":"_QMXb08jTRHL"},"source":["## Exercice 1\n","1. Train a robust classifier using Adversarial Training\n","2. Evaluate your classifier on natural and adversarial examples\n","3. Make an analysis and conclude"]},{"cell_type":"markdown","metadata":{"id":"pejwlPboMtOj"},"source":["## Loading MNIST dataset (train set and test set)"]},{"cell_type":"code","metadata":{"id":"w21alYjaTRHL","executionInfo":{"status":"ok","timestamp":1607679169635,"user_tz":-60,"elapsed":859,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 3\n","\n","# load MNIST dataset\n","def load_mnist(split, batch_size):\n","  train = True if split == 'train' else False\n","  dataset = datasets.MNIST(\"./data\", train=split, download=True, transform=transforms.ToTensor())\n","  return DataLoader(dataset, batch_size=batch_size, shuffle=train)\n","\n","batch_size = 100\n","train_loader = load_mnist('train', batch_size)\n","test_loader = load_mnist('test', batch_size)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcYrp0VvB-tC"},"source":["## Implementing FGSM and PGD\n","\n","Nous reprenons le codage des classes réalisé dans le Notebook précédent."]},{"cell_type":"code","metadata":{"id":"j9B-ea_dTRHO","executionInfo":{"status":"ok","timestamp":1607679169995,"user_tz":-60,"elapsed":1213,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 4\n","\n","class FastGradientSignMethod:\n","  def __init__(self, model, eps):\n","    self.model = model\n","    self.eps = eps\n","  \n","  def compute(self, x, y):\n","    \"\"\" Construct FGSM adversarial perturbation for examples x\"\"\"    \n","    x.requires_grad=True # enable locally gradient computation on x\n","    \n","    output = self.model(x)\n","    loss_func = nn.CrossEntropyLoss() # define a specific loss to compute the gradient w.r.t x\n","    loss = loss_func(output, y)\n","    loss.backward() # back-propagate the gradient w.r.t x\n","\n","    delta = self.eps * x.grad.sign() # compute the delta of pertubation by applying the sign of the gradient of x\n","    x.requires_grad=False # disable gradient computation on x\n","    \n","    return x + delta # return the attacked (modified) image\n","\n","\n","class ProjectedGradientDescent:\n","  \n","  def __init__(self, model, eps, alpha, num_iter):\n","    self.model = model\n","    self.eps = eps\n","    self.alpha = alpha\n","    self.num_iter = num_iter\n","  \n","  def compute(self, x, y):\n","    # we define a specific loss to compute the gradient w.r.t x\n","    loss_func = nn.CrossEntropyLoss()\n","    \n","    # then initialize the delta to 0 with the same shape as x\n","    delta = torch.zeros_like(x, requires_grad=True)\n","\n","    # and perform gradient descent iterative procedure\n","    for i in range(self.num_iter):\n","      # we compute the output image from the modified image\n","      output = self.model(x + delta)\n","\n","      # evaluate the loss on the outup\n","      loss = loss_func(output, y)\n","      \n","      # back-propagate the gradient w.r.t delta\n","      loss.backward()\n","\n","      # update the delta with it current gradient\n","      delta.data += self.alpha * delta.grad.data\n","\n","      # clip the delta in the range [-eps, eps]\n","      delta.data = delta.data.clamp(-self.eps, self.eps)\n","\n","      # reset the gradient on delta\n","      delta.grad.zero_()\n","\n","    return x + delta.detach() # return the modified (attacked) image\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RV7GnBudCcpr"},"source":["## Implementing convolutional network\n","\n","Nous reprenons également le codage du modèle convolutif réalisé dans le Notebook précédent."]},{"cell_type":"code","metadata":{"id":"gJksQokbTRHQ","executionInfo":{"status":"ok","timestamp":1607679169997,"user_tz":-60,"elapsed":1208,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 5\n","\n","# define a neural network with 3 fully connected layers\n","class FullyConnectedModel(torch.nn.Module):\n","  \n","  def __init__(self, input_dim, output_dim):\n","    super(FullyConnectedModel, self).__init__()\n","    self.layers = nn.Sequential(nn.Flatten(), \n","                                nn.Linear(input_dim, 256), nn.ReLU(),\n","                                nn.Linear(256, 64), nn.ReLU(),\n","                                nn.Linear(64, output_dim)\n","    )\n","        \n","  def forward(self, x):\n","    return self.layers(x)\n","    \n","# convolutional model with 2 convolution, 2 max pooling layer, 3 fully connected layer\n","# the model should be: 2x (conv -> max pooling -> relu) -> 2x (fc -> relu) -> fc\n","class ConvModel(torch.nn.Module):\n","    def __init__(self):\n","        super(ConvModel, self).__init__()\n","        \n","        # 2 convolutional layers + 3 fully connected layers\n","        self.layers = nn.Sequential(\n","            # 1st conv layer\n","            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1), # -> input=8, output=28\n","            nn.MaxPool2d(kernel_size=2), nn.ReLU(), # => output = 28/2 = 14\n","            \n","            #2nd conv layer\n","            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1, padding=1), # input=14, output=14\n","            nn.MaxPool2d(kernel_size=2), nn.ReLU(), # => output = 14/2 = 7\n","            \n","            # we use the fully connected layers defined above\n","            FullyConnectedModel(16*7*7, 10)\n","        ) \n","    \n","    def forward(self, x):\n","        return self.layers(x)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL3VfKQ_HuDW"},"source":["## Adversarial training on train set\n","\n","La fonction `adversarial_train_model` reprend pour l'essentiel la fonction d'entraînement `train_model` développée dans le Notebook 2. La seule différence réside dans le fait que la nouvelle fonction nécessite un paramètre supplémentaire `attack` permettant de lui passer le type d'attaque réalisée. Ceci ce traduit dans le corps de la fonction par la présence, en cas d'attaque :\n","- d'une nouvelle instruction `delta = attack.compute(imgs, labels)` destinée à récupérer le tenseur des perturbations causées par l'attaque ;\n","- d'une instruction `imgs = imgs + delta` qui ajoute à l'image d'origine les valeurs des perturbations.\n","\n","C'est sur la base d'une image perturbée (ou pas si le paramètre `attack`n'est pas renseigné) qu'est ensuite réalisé l'entraînement du modèle. En cas d'attaque, comme précisé dans l'énoncé, l'ordre des opérations est le suivant :\n","\n","1. L'attaquant prend connaissance du vecteur de paramètres $\\theta$ (attaque de type white box) ;\n","2. Connaissant $\\theta$, il en déduit la valeur de $\\delta$ qui permet de maximiser la loss, dans les limites fixées par le paramètre $\\epsilon$ ;\n","3. Sachant le $\\delta$ utilisé par l'attaquant, la défense consiste à ajuster le vecteur de paramètres $\\theta$ de façon à minimiser la loss.\n","\n","Le déroulé du programme qui en découle est le suivant :\n","1. Calcul `delta = attack.compute(imgs, labels)` du $\\delta$ utilisé par l'attaquant ;\n","2. Perturbation `imgs = imgs + delta` de l'image initiale découlant de la valeur de $\\delta$ utilisée ;\n","3. Apprentissage sur l'image bruitée (lignes `optimizer.zero_grad()` à `optimizer.step()`) pour déterminer le nouveau vecteur de paramètres $\\theta$ optimal après attaque."]},{"cell_type":"code","metadata":{"id":"ZJM3XkrFTRHT","executionInfo":{"status":"ok","timestamp":1607679169999,"user_tz":-60,"elapsed":1203,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 6\n","\n","def adversarial_train_model(model, criterion, optimizer, loader, attack):\n","  \"\"\"\n","    Function to train the model in adverserial way. The training is quiet different from the standard one.\n","    In fact, the optimisation we'll be performed on the attacked image.\n","    Therefore, we have 2 steps in our optimisation procedure:\n","      - compute the adverserial image (Max)\n","      - optimize the objective function (loss) on the adversarial image (Min)\n","  \"\"\"\n","  losses = []\n","  for epoch in range(10):\n","    for imgs, labels in loader:\n","      imgs, labels = imgs.to(device), labels.to(device)\n","      \n","      # in case attack is provided, we modify (MAX step) the original before optimizing the bjective\n","      if attack:\n","        imgs = attack.compute(imgs, labels)\n","\n","      # performing the optimization on the potential modified image (MIN step)\n","      output = model(imgs)\n","      loss = criterion(output, labels)\n","\n","      if optimizer:\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","      \n","      # backup the loss for furture analysis\n","      losses.append(loss.detach().cpu().numpy())\n","\n","    print('epoch {}, loss: {:.4f}'.format(epoch, losses[-1]))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMEKrqNhUkyV"},"source":["#### Training parameters"]},{"cell_type":"code","metadata":{"id":"btLvocEsUlD-","executionInfo":{"status":"ok","timestamp":1607679170000,"user_tz":-60,"elapsed":1197,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# define your loss\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# epsilon\n","epsilon = 0.1"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0hsFmcwVauL"},"source":["### Training convolution network under FGSM"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9XiAdGaVa4e","executionInfo":{"status":"ok","timestamp":1607679243849,"user_tz":-60,"elapsed":75038,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}},"outputId":"5ab76868-8abb-4600-86fb-4c4801deb230"},"source":["# we define the conv model trained under FGSM\n","fgsm_model = ConvModel().to(device)\n","\n","# define the optimizer for FGSM training\n","fgsm_optimizer = torch.optim.SGD(fgsm_model.parameters(), lr=0.1)\n","\n","# define FGSM attack object\n","fgsm_attack = FastGradientSignMethod(fgsm_model, epsilon)\n","\n","# train our model with FGSM perturbation\n","adversarial_train_model(fgsm_model, criterion, fgsm_optimizer, train_loader, fgsm_attack)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["epoch 0, loss: 0.4265\n","epoch 1, loss: 0.2300\n","epoch 2, loss: 0.4367\n","epoch 3, loss: 0.1628\n","epoch 4, loss: 0.2528\n","epoch 5, loss: 0.1967\n","epoch 6, loss: 0.0939\n","epoch 7, loss: 0.1131\n","epoch 8, loss: 0.1238\n","epoch 9, loss: 0.1257\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t6iiQDjBWTHP"},"source":["### Training convolution network under PGD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKSphIbuWTQ4","executionInfo":{"status":"ok","timestamp":1607679359505,"user_tz":-60,"elapsed":190683,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}},"outputId":"ee36cda3-94ce-4cca-fc74-7d5f3f70e821"},"source":["# we define PGD the hyperparameters\n","alpha = 1e-2\n","num_iter = 5\n","\n","# we define the conv model trained under PGD\n","pgd_model = ConvModel().to(device)\n","\n","# define the optimizer for FGSM training\n","pgd_optimizer = torch.optim.SGD(pgd_model.parameters(), lr=0.1)\n","\n","# define PGD attack object\n","pgd_attack = ProjectedGradientDescent(model=pgd_model, eps=epsilon, alpha=alpha, num_iter=num_iter )\n","\n","# train our model with PGD perturbation\n","adversarial_train_model(pgd_model, criterion, pgd_optimizer, train_loader, pgd_attack)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["epoch 0, loss: 0.2651\n","epoch 1, loss: 0.2035\n","epoch 2, loss: 0.0961\n","epoch 3, loss: 0.1013\n","epoch 4, loss: 0.1465\n","epoch 5, loss: 0.0897\n","epoch 6, loss: 0.1039\n","epoch 7, loss: 0.0426\n","epoch 8, loss: 0.0186\n","epoch 9, loss: 0.1586\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N1hKdvfsRQk0"},"source":["## Evaluating our model on the test data\n","\n","Le réseau est tour à tour attaqué grâce aux méthodes FGSM puis PGD.\n","\n","L'apprentissage adversarial est réalisé pour chaque attaque sur le jeu d'entraînement. Une fois l'apprentissage réalisé, l'appel de la fonction d'évaluation sur le jeu de test permet de calculer l'accuracy du réseau entrainé sur les données perturbées de ce jeu.\n","\n","A noter :\n","- le critère et l'optimiseur sont déclarés une seule fois dan la mesure où ils sont identiques quel que soit le type d'attaque réalisé ;\n","- on instancie le réseau convolutif une première fois via `model = ConvModel()` pour l'attaque FGSM et une seconde fois via `model2 = ConvModel()` de sorte à ce que la seconde attaque (PGD) ne soit pas réalisée sur le modèle déjà entraîné lors de la première attaque (FGSM).  "]},{"cell_type":"code","metadata":{"id":"2C3VJ0qtTRHW","executionInfo":{"status":"ok","timestamp":1607679359506,"user_tz":-60,"elapsed":190677,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}}},"source":["# [CELL ID] 7\n","\n","def eval_model(model, loader, attack=None):\n","  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n","  correct, n = 0, 0\n","  for imgs, labels in loader:\n","    imgs, labels = imgs.to(device), labels.to(device)\n","    \n","    if attack:\n","      imgs = attack.compute(imgs, labels)\n","\n","    output = model(imgs)\n","    \n","    # transforming the output to probability by applying log_softmax function\n","    proba = F.log_softmax(output, dim=1)\n","    pred = proba.max(1, keepdim=True)[1]\n","\n","    correct += torch.sum(pred.squeeze()==labels).item()\n","    n += pred.shape[0]\n","\n","  return correct/n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YeVdipQSc7d"},"source":["### Evaluating under FGSM attack"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2mhLSu_O98B","executionInfo":{"status":"ok","timestamp":1607679371051,"user_tz":-60,"elapsed":202215,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}},"outputId":"2ee64278-9a26-49c1-f7cf-045730ad45b6"},"source":["# we evaluate the model trained under FGSM on the original test set\n","normal_acc = eval_model(fgsm_model, test_loader, attack=None)\n","\n","# we evaluate the model trained under FGSM on the test set under FGSM attack\n","attack_acc = eval_model(fgsm_model, test_loader, attack=fgsm_attack)\n","\n","\n","print(\"accuracy FGSM model vs NO attack::\", normal_acc)\n","print(\"accuracy FGSM model vs FGSM attack::\", attack_acc)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["accuracy FGSM model vs NO attack:: 0.9960333333333333\n","accuracy FGSM model vs FGSM attack:: 0.9727166666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ISRlxqIVShai"},"source":["### Evaluating under PGD attack"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVxr5g6PSknW","executionInfo":{"status":"ok","timestamp":1607679386045,"user_tz":-60,"elapsed":217201,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}},"outputId":"d01ef4b6-d791-4db7-eb47-df68330a0a8d"},"source":["# we evaluate the model trained under PGD on the original test set\n","normal_acc = eval_model(pgd_model, test_loader, attack=None)\n","\n","# we evaluate the model trained under PGD on the test set under PGD attack\n","attack_acc = eval_model(pgd_model, test_loader, attack=pgd_attack)\n","\n","print(\"accuracy PGD model vs NO attack::\", normal_acc)\n","print(\"accuracy PGD model vs PGD attack::\", attack_acc)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["accuracy PGD model vs NO attack:: 0.9751666666666666\n","accuracy PGD model vs PGD attack:: 0.9738666666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qKa6ekTf14y3"},"source":["### Evaluating under cross-attack\n","Nous allons dans cette section évaluer l'impact d'une cross-attack, c'est-à-dire, évaluer la performance d'un modèle entrainé contre une attaque FGSM avec des images modifiées par PGD et vice-versa.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5jeTiai15BG","executionInfo":{"status":"ok","timestamp":1607679402817,"user_tz":-60,"elapsed":233964,"user":{"displayName":"Kodjo Klouvi","photoUrl":"","userId":"17313844943151484087"}},"outputId":"b40d3534-dcbc-42c2-e070-c09804f8ce3e"},"source":["# we evaluate the model trained under FGSM on the test set under PGD attack\n","fgsm_attack_acc = eval_model(fgsm_model, test_loader, attack=pgd_attack)\n","\n","# we evaluate the model trained under PGD on the test set under FGSM attack\n","pgd_attack_acc = eval_model(pgd_model, test_loader, attack=fgsm_attack)\n","\n","print(\"accuracy FGSM model vs PGD attack:\", fgsm_attack_acc)\n","print(\"accuracy PGD model vs FGSM attack:\", pgd_attack_acc)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["accuracy FGSM model vs PGD attack: 0.9960333333333333\n","accuracy PGD model vs FGSM attack: 0.9374166666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uz1CtYbd0g8M"},"source":["## Conclusion\n","\n","Les niveaux d'accuracy sur les données d'origine sont de l'ordre de 99%. Nous avons vu dans le notebook 2 que dans le cas du modèle convolutif (celui utilisé dans ce Notebook) les niveaux d'accuracy baissaient à 75% pour l'attaque FGSM et à 85% pour l'attaque PGD. Avec entraînement adversarial, les niveaux d'accucary sont de 95.12% après une attaque FGSM et de 95.61% après une attage PGD. Même si ces niveaux restent inférieurs à ceux obtenus sur les données d'origine, la baisse constatée est faible, preuve que l'entraînement adversarial a permis d'augmenter de façon très significative la robustesse du modèle à une attaque."]}]}