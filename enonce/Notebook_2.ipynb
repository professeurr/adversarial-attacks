{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Notebook_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"2VUv9a_BTvGy"},"source":["# [CELL ID] 1\n","\n","import numpy as np\n","import scipy as sp\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XnJzSOTGTvG2"},"source":["[CELL ID] 2\n","## Moving to neural networks\n","\n","Now that we've seen how adversarial examples work in the context of linear models, let's move to the setting we really care about: the possibility of adversarial examples in deep neural networks. \n","\n","First, let us define formally a neural network. Let us define $h_\\theta(x) : \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$ to be an $d$-layer network, given by the following equations\n","\\begin{equation}\n","\\begin{split}\n","z_1 & = x \\\\\n","z_{i+1} & = f_i(W_i z_i + b_i), \\;\\; i,\\ldots,d \\\\\n","h_\\theta(x) & = z_{d+1}\n","\\end{split}\n","\\end{equation}\n","where $z_i$ denote the activations at layer $i$; $f_i$ denotes the activation function for layer $i$, which we will often take to be e.g. the ReLU operator $f_i(z) = \\max\\{0,z\\}$ for layers $i=1,\\ldots,d-1$ and the linear operator $f_i(z) = z$ for layer $d$ (remember, the last layer outputs are the class logits, and the loss function \"builds in\" the softmax operator, so we don't explicitly put the softmax into the network); and the parameters of the network are given by $\\theta = \\{W_1,b_1,\\ldots,W_d,b_d\\}$ (in the above, $W_i$ is most obviously interpreted a matrix, but it could really be any linear operator including convolutions).  We won't worry for now about explicitly defining the size of all the intermediate layers, but we assume that they are specified such that the final function $h_\\theta$ takes inputs in $\\mathbb{R}^n$ and outputs vectors in $\\mathbb{R}^k$.  The loss function for multiclass case, namely the Cross Entropy loss, is as follows:\n","\\begin{equation}\n","\\ell(h_\\theta(x), y) = \\log \\left ( \\sum_{j=1}^k \\exp(h_\\theta(x)_j) \\right ) - h_\\theta(x)_y.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"aQcwbjuOTvG3"},"source":["[CELL ID] 3\n","## Crafting an attack\n","\n","Unlike it the linear case, it is _not_ easy to solve, the maximization problem over our perturbation. This is because the cost surface for neural networks (here considering it over the _input_ space, not parameter space) is not convex, and is especially prone to local optima.  We can see this even in the simple example above, where there are local optimal when either trying to maximize or minimize the function above, and the initial gradient at a given point may or may not point in the direction of the actual maxima. \n","\n","So how _do_ we go about (approximately) solving the inner optimization problem\n","\\begin{equation}\n","\\DeclareMathOperator*{\\maximize}{maximize}\n","\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y)\n","\\end{equation}\n","in the case of $h_\\theta$ being a neural network? \n","\n","The most common strategy to approximate the pertubation is to find a _lower bound_ on the optimization objective.  Because (by definition) _any_ feasible $\\delta$ will give us a lower bound, this is equivalent to just \"trying to empirically solve the optimization problem\", i.e., \"find an adversarial example.\"  \n","This is by far the most common strategy for solving the inner maximization motivated largely by the fact that for neural networks in general, problems of local optima don't seem as bad as initially may be thought. "]},{"cell_type":"markdown","metadata":{"id":"5wyZe7amTvG3"},"source":["[CELL ID] 4\n","## Lower bounding the inner maximization (i.e., adversarial attacks)\n","\n","Let's start by considering perhaps the simplest way of solving the optimization problem we care about, the task of maximizing\n","\\begin{equation}\n","\\DeclareMathOperator*{\\maximize}{maximize}\n","\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n","\\end{equation}\n","In fact we already saw one example for how to do this in our introductory lecture, but the basic idea here is quite simple: using backpropagation, we can compute the gradient of the loss function with respect to the perturbation $\\delta$ itself, so let's just perform gradient descent on $\\delta$ to maximize our objective.  However, we also need to ensure that $\\delta$ stays within the norm bound $\\epsilon$, so after each step, we can project back into this space.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dlx4W6smTvG4"},"source":["[CELL ID 5]\n","### The Fast Gradient Sign Method (FGSM)\n","\n","Let's consider in a bit more detail how we might do the attack we mentioned above.  For some given example $x$, we know that we are going to adjust $\\delta$ in the direction of it's gradient, i.e., we will first compute the gradient\n","\\begin{equation}\n","g := \\nabla_\\delta \\ell(h_\\theta(x + \\delta),y)\n","\\end{equation}\n","using simple backpropagation; note that if we're evaluating this gradient at $\\delta=0$ (as we would at the first timestep), then this is also just equal to $\\nabla_x \\ell(h_\\theta(x),y)$, but we'll express the gradients in terms of $\\delta$ to be consistent with how we do things going forward.\n","\n","In order to maximize loss, we want to adjust delta in the direction of this gradient, i.e., take a step\n","\\begin{equation}\n","\\delta := \\delta + \\alpha g\n","\\end{equation}\n","for some step size $\\alpha$ and then project back into the norm ball defined by $\\|\\delta\\| \\leq \\epsilon$.  But how big a step do we take?  For concreteness, let's consider the particular case of the $\\ell_\\infty$ norm $\\|\\delta\\|_\\infty \\leq \\epsilon$, where, as we mentioned before, projecting onto this norm ball simply involves clipping values of $\\delta$ to lie within the range $[-\\epsilon, \\epsilon]$.  If our initial $\\delta$ is zero, this gives the update\n","\\begin{equation}\n","\\delta := \\mathrm{clip}(\\alpha g, [-\\epsilon, \\epsilon]).\n","\\end{equation}\n","Now, how big of a step size should we take?  If we want to make increase the loss as much as possible, it makes sense to take as large a step as possible, i.e., take $\\alpha$ to be very large (of course knowing that we won't take _that_ big a step, since we're projecting back into the $\\ell_\\infty$ ball afterwards).  It is not hard to see that for $\\alpha$ large enough, the relative sizes of the entries of $g$ won't matter: we will simply take $\\delta_i$ to be either $+\\epsilon$ or $-\\epsilon$ depending upon the sign of $g_i$.  In other words, for large $\\alpha$, this update becomes\n","\\begin{equation}\n","\\delta := \\epsilon \\cdot \\mathrm{sign}(g).\n","\\end{equation}\n","\n","This is know as the Fast Gradient Sign Method (FGSM) [1], and it was one of the first methods for constructing adversarial examples proposed by the deep learning community.\n","\n","[1] Explaining and Harnessing Adversarial Examples"]},{"cell_type":"markdown","metadata":{"id":"gjt8kxJfTvG4"},"source":["### Exercice 2\n","1. Define a neural network with 3 fully connected layers\n","2. Define a neural network with 2 convolutional layers and a last fully connected layer\n","2. Train the network on the full MNIST Dataset\n","3. Code the FGSM attack\n","4. Evaluate your model against FGSM\n","5. Plot some images with and without the adversarial perturbation\n","6. Make an analysis of FGSM attack and compare it with the attack from the first notebook"]},{"cell_type":"code","metadata":{"id":"E6LX9ZDNTvG5"},"source":["# [CELL ID] 6\n","\n","# load MNIST dataset\n","def load_mnist(split, batch_size):\n","  train = True if split == 'train' else False\n","  dataset = datasets.MNIST(\"./data\", train=split, download=True, transform=transforms.ToTensor())\n","  return DataLoader(dataset, batch_size=batch_size, shuffle=train)\n","\n","batch_size = 100\n","train_loader = load_mnist('train', batch_size)\n","test_loader = load_mnist('test', batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJdE1uDbTvG7"},"source":["# [CELL ID] 7\n","\n","def train_model(model, criterion, optimizer, loader):\n","  \"\"\"Function to train the model\"\"\"\n","  # code here ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chUE0uAlTvG-"},"source":["# [CELL ID] 8\n","\n","# define a neural network with 3 fully connected layers\n","class FullyConnectedModel(torch.nn.Module):\n","  \n","  def __init__(self):\n","    super(FullyConnectedModel, self).__init__()\n","    # code here ...\n","  \n","  def forward(self, x):\n","    # code here ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1TM0cVUTvHA"},"source":["# [CELL ID] 9\n","\n","# convolutional model with 2 convolution, 2 max pooling layer, 3 fully connected layer\n","# the model should be: 2x (conv -> max pooling -> relu) -> 2x (fc -> relu) -> fc\n","\n","class ConvModel(torch.nn.Module):\n","  \n","  def __init__(self):\n","    super(ConvModel, self).__init__()\n","    # code here ...\n","  \n","  def forward(self, x):\n","    # code here ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLS4ty1sTvHC"},"source":["# [CELL ID] 10\n","\n","fc_model = FullyConnectedModel()\n","fc_model = fc_model.cuda()\n","\n","# define your loss function\n","criterion = # code here ...\n","\n","# define your optimizer\n","opt = # code here ...\n","\n","train_model(fc_model, criterion, opt, train_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVeRox2XTvHG"},"source":["# [CELL ID] 11\n","\n","conv_model = ConvModel()\n","conv_model = conv_model.cuda()\n","\n","# define your loss function\n","criterion = # code here ...\n","\n","# define your optimizer\n","opt = # code here ...\n","\n","train_model(conv_model, criterion, opt, train_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfZpqvldTvHI"},"source":["# [CELL ID] 12\n","\n","def eval_model(model, loader, attack=None):\n","  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n","  # code here ...\n","\n","eval_model(fc_model, test_loader)\n","eval_model(conv_model, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-G90kSITvHK"},"source":["# [CELL ID] 13\n","\n","class FastGradientSignMethod:\n","  \n","  def __init__(self, model, eps):\n","    # code here ...\n","  \n","  def compute(self, x, y):\n","    \"\"\" Construct FGSM adversarial perturbation for examples x\"\"\"\n","    # code here ...\n","\n","fgsm = # code here ...\n","eval_model(fc_model, test_loader, fgsm)\n","\n","fgsm = # code here ...\n","eval_model(conv_model, test_loader, fgsm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTKKRESkTvHM"},"source":["# [CELL ID] 14\n","\n","def plot_images(x, y, yp, M=5, N=5):\n","  x = x.cpu().numpy()\n","  y = y.cpu().numpy()\n","  f, ax = plt.subplots(M, N, sharex=True, sharey=True, figsize=(N,M*1.3))\n","  for i in range(M):\n","    for j in range(N):\n","      ax[i][j].imshow(1-x[i*N+j][0], cmap=\"gray\")\n","      title = ax[i][j].set_title(\"Pred: {}\".format(yp[i*N+j].max(dim=0)[1]))\n","      plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))\n","      ax[i][j].set_axis_off()\n","    plt.tight_layout()\n","  plt.show()\n","  \n","for imgs, labels in test_loader:\n","  imgs, labels = imgs.cuda(), labels.cuda()\n","  break\n","\n","delta = fgsm.compute(imgs, labels)\n","output = conv_model(imgs + delta)\n","plot_images(imgs+delta, labels, output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51gHv7p5TvHO"},"source":["[CELL ID] 15\n","\n","### Projected gradient descent\n","\n","This discussion immediately brings us to the next approach we would want to consider for maximizing the optimization problem, just doing projected gradient descent (i.e., the above procedure, but iterating it and with a smaller step size). [Note: technically speaking, this is gradient _ascent_ since we are maximizing a function rather than minimization, but it's common to just refer to the process as gradient descent.]. This was also called the \"basic iterative procedure\" in it's first appearance within the adversarial attacks literature [1], but essentially it is just the obvious choice for how we might maximize the inner objective a bit more carefully than with FGSM. The basic PGD algorithm simply iterates the updates:\n","\\begin{equation}\n","\\begin{split}\n","& \\mbox{Repeat:} \\\\\n","& \\quad \\delta := \\mathcal{P}(\\delta + \\alpha \\nabla_\\delta \\ell(h_\\theta(x+\\delta), y))\n","\\end{split}\n","\\end{equation}\n","where $\\mathcal{P}$ denotes the projection onto the ball of interest (for example, clipping in the case of the $\\ell_\\infty$ norm).  Of course, with PGD we now have more choices we need to make when specifying the attack, such as the actual stepsize itself, and the number of iterations (we'll discuss some rules of thumb shortly once we present a slightly modified version of the above iteration).  We can implement this attack in the following manner, where we're here going to implement the gradient descent procedure rather than rely on one of PyTorch's optimizers, as we want to see what's going on a bit more explicitly (PyTorch's SGD also includes terms like momentum, which actually usually are able to optimize the inner term _better_, but we want to have as little black-box procedures here as possible).\n","\n","[1] Adversarial Machine Learning At Scale"]},{"cell_type":"code","metadata":{"id":"s6Ayp36hTvHP"},"source":["# [CELL ID] 16\n","\n","class ProjectedGradientDescent:\n","  \n","  def __init__(self, model, eps, alpha, num_iter):\n","    # code here ...\n","  \n","  def compute(self, x, y):\n","    \"\"\" Construct PGD adversarial pertubration on the examples x.\"\"\"  \n","    # code here ...\n","\n","\n","pgd = # code here ...\n","eval_model(fc_model, test_loader, pgd)\n","\n","pgd = # code here ...\n","eval_model(conv_model, test_loader, pgd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6y6okpoETvHS"},"source":["pgd = # code here ...\n","delta = pgd.compute(imgs, labels)\n","output = conv_model(imgs + delta)\n","plot_images(imgs+delta, labels, output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKnFIK-2TvHU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdANrvWvTvHW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nGqm2W8TvHY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iHGLqONTvHb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWcvN4XJTvHe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uCiN-CiTvHg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BM1YkEf9TvHi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOd9JYvQTvHk"},"source":[""],"execution_count":null,"outputs":[]}]}